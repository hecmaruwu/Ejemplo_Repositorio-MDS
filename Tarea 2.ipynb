{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "tWgUwM93HfoT",
      "metadata": {
        "id": "tWgUwM93HfoT"
      },
      "source": [
        "# Tarea 2: Redes Generativas Adversarias\n",
        "\n",
        "### MDS7203 Modelos Generativos Profundos\n",
        "\n",
        "**Nombre:**\n",
        "- Carolina Nuñez\n",
        "- Hecmar Taucare\n",
        "\n",
        "**Fecha de entrega:**\n",
        "\n",
        "En esta segunda tarea, el objetivo es implementar el modelo [Pix2Pix](https://arxiv.org/abs/1611.07004), el cual es un modelo tipo GAN que permite realizar tareas de image-to-image translation. Dado que las GANs suelen sufrir de inestabilidades durante el entrenamiento, esta tarea evaluará, además, el uso de heurísticas sencillas que se suelen usar al entrenar este tipo de modelos.\n",
        "\n",
        "Algunas instrucciones generales:\n",
        "- Se sugiere usar herramientas como ChatGPT o Claude, entre otras.\n",
        "- Para la entrega, no es necesario un informe, este archivo es suficiente.\n",
        "- Se debe entregar el documento con todas las celdas ejecutadas.\n",
        "- La tarea debe ser realizada en Google Colab.\n",
        "- La tarea tiene 3 partes, y cada parte vale lo mismo.\n",
        "- Se recomienda leer la tarea completa antes de comenzar su desarrollo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "swycRifyNtiw",
      "metadata": {
        "id": "swycRifyNtiw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import gdown\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "n0H74eqXM45e",
      "metadata": {
        "id": "n0H74eqXM45e"
      },
      "source": [
        "## Parte 1 (datos de entrenamiento)\n",
        "\n",
        "En esta primera parte se implementará la clase `MapsDataset` que se utilizará para definir el dataset de entrenamiento para el modelo `pix2pix`. Los datos de entrenamiento que se usarán en este modelo corresponden al dataset `maps`, el cual fue construido especialmente para el modelo `pix2pix`. Cada muestra de este dataset consiste en pares de imágenes de la forma $(x_0,x_1)$, donde $x_0$ es una foto aérea de alguna región de Nueva York y $x_1$ es la imagen de un mapa asociado a la región que cubre $x_1$.\n",
        "\n",
        "Con este dataset definido, el objetivo en las partes 2 y 3 de la tarea será implementar y entrenar un modelo `pix2pix` que aprenda a transformar una imagen satelital $x_0$ dada como entrada en su mapa $x_1$ homólogo.\n",
        "\n",
        "### Descarga"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gD1IHjh19PGc",
      "metadata": {
        "id": "gD1IHjh19PGc"
      },
      "outputs": [],
      "source": [
        "data_link = 'https://drive.google.com/file/d/1vWAntLe0m3lkRXjXss_j3oHWULCC0btk/view?usp=share_link'\n",
        "\n",
        "gdown.download(data_link, 'maps.zip', quiet=False, fuzzy=True)\n",
        "\n",
        "with zipfile.ZipFile('maps.zip', 'r') as zip_data:\n",
        "    zip_data.extractall('/content/data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Zp64okgsi3x0",
      "metadata": {
        "id": "Zp64okgsi3x0"
      },
      "outputs": [],
      "source": [
        "# Ejemplo:\n",
        "img = Image.open('/content/data/maps/train/1.jpg')\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8pTa1WOePvMn",
      "metadata": {
        "id": "8pTa1WOePvMn"
      },
      "source": [
        "- ¿Cómo fue construido el dataset `maps`? Esta información se puede encontrar en el paper del modelo `pix2pix`.\n",
        "> **Respuesta:**\n",
        "  El dataset _maps_ fue construido a partir de imágenes aéreas de Nueva York y alrededores extraídas de Google Maps.\n",
        "\n",
        "- Para descargar los datos se usó la biblioteca `gdown`, pero se podría haber montado la unidad de Google Drive para hacerlo. ¿Cuál es la ventaja y desventaja de cada enfoque? ¿Cuándo convendría montar la unidad de Google Drive en vez de usar `gdown`?\n",
        "> **Respuesta:**\n",
        "  - _Ventajas:_\n",
        "  1. Evita configuraciones manuales lo que resulta útil en caso de descargar archivos de gran tamaño.\n",
        "  2. Fácil de usar.\n",
        "  3. Facilita la automatización, i.e facilita el acceso de scripts a recursos externos.\n",
        "  - _Desventajas:_\n",
        "  1. depende de Google Drive, i.e de la ubicación de los archivos en la nube.\n",
        "  2. Tiene límite de descargas, pudiendo generar inconvenientes a los usuarios un archivo si se descarga muchas vces.\n",
        "  3.  No funciona con carpetas directamente (se deben descargar los archivos por separado). \\\\\n",
        "  - Dado que _gdown_ permite descargar archivos o carpetas de un Drive público, cuando se quiera acceder a archivos privados se debería montar una unidad de Google Drive.\n",
        "\n",
        "\n",
        "\n",
        "### Transformaciones para data augmentation\n",
        "\n",
        "Se observa que cada archivo de imagen consiste en la concatenación horizontal de las imágenes $x_0$ y $x_1$, por lo que hay que separar el par de imágenes $(x_0,x_1)$ manualmente. Esto se realizará posteriormente en la clase `MapsDataset`. Antes de esto, se debe implementar la clase `PairedTransforms`, cuyo objetivo es aplicar una transformación (p.g. para data augmentation) de forma sincronizada sobre un par de muestras $(x_0,x_1)$, lo cual es necesario para un buen entrenamiento (p.g., si $x_0$ se refleja horizontalmente, $x_1$ también debe hacerlo).\n",
        "\n",
        "- Implemente el método `__call__` de la clase `PairedTransforms`, la cual busca aplicar una misma transformación (p.g. `transforms.ToTensor`) a dos imágenes. Notar que algunas transformaciones tienen una componente aleatoria (p.g. `transforms.RandomCrop`), la cual se debe fijar para aplicar exactamente la misma transformación sobre $x_0$ y $x_1$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Dbn_R7R3hrqg",
      "metadata": {
        "id": "Dbn_R7R3hrqg"
      },
      "outputs": [],
      "source": [
        "class PairedTransforms:\n",
        "    # clase para data augmentation\n",
        "    def __init__(self, transform):\n",
        "        self.transform = transform\n",
        "\n",
        "    def __call__(self, x0, x1):\n",
        "      # aplica una transformacion a 2 imágenes\n",
        "\n",
        "      # Uso una semilla por si la transformación tiene una componente aleatoria\n",
        "      seed = torch.seed()\n",
        "      random.seed(seed)\n",
        "      torch.manual_seed(seed)\n",
        "\n",
        "      x0 = self.transform(x0)\n",
        "      x1 = self.transform(x1)\n",
        "        return x0, x1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIInCXP20iXB"
      },
      "outputs": [],
      "source": [
        "# class PairedTransforms:\n",
        "#     # clase para data augmentation\n",
        "#     def __init__(self, transform):\n",
        "#         self.transform = transform\n",
        "\n",
        "#     def __call__(self, x0, x1):\n",
        "#       # aplica una transformacion a 2 imágenes\n",
        "#       # ToTensor() garantiza que las imagenes estén en el formato tensorial }\n",
        "#       # necesario para trabajar con PyTorch\n",
        "\n",
        "#       # RandomCrop tecnica de eliminacion de areas exteriores no deseadas de una\n",
        "#       # imagen\n",
        "#       i, j, h, w = transforms.RandomCrop.get_params(x0,\n",
        "#                                                         output_size=(256, 256))\n",
        "#       # Se aplica el mismo RandomCrop a ambas imágenes\n",
        "#       x0 =  transforms.functional.crop(x0, i, j, h, w)\n",
        "#       x1 =  transforms.functional.crop(x1, i, j, h, w)\n",
        "\n",
        "#       x0 = transforms.ToTensor(x0)\n",
        "#       x1 = transforms.ToTensor(x1)\n",
        "\n",
        "#         return x0, x1"
      ],
      "id": "vIInCXP20iXB"
    },
    {
      "cell_type": "markdown",
      "id": "B19lgpmkATX8",
      "metadata": {
        "id": "B19lgpmkATX8"
      },
      "source": [
        "### Clase `MapsDataset`\n",
        "\n",
        "La siguiente clase permite obtener pares de entrenamiento $(x_0,x_1)$ para poder entrenar el modelo `pix2pix`. El método `__getitem__` divide la imagen obtenida en $(x_0, x_1)$ y luego aplica la transformación `transform` recibida en el constructor al par de imágenes $(x_0,x_1)$ utilizando la clase `PairedTransforms` recién implementada:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f64638d1",
      "metadata": {
        "id": "f64638d1"
      },
      "outputs": [],
      "source": [
        "class MapsDataset(Dataset):\n",
        "\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.data_filenames = [file for file in os.listdir(root_dir) if file.endswith('jpg')]\n",
        "        self.transform = PairedTransforms(transform)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_filenames)\n",
        "\n",
        "    def __getitem__(self, n):\n",
        "        img_path = os.path.join(self.root_dir, self.data_filenames[n])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        w, h = image.size\n",
        "        x0 = image.crop((0, 0, w//2, h))\n",
        "        x1 = image.crop((w//2, 0, w, h))\n",
        "\n",
        "        if self.transform:\n",
        "            x0, x1 = self.transform(x0, x1)\n",
        "\n",
        "        return x0, x1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lwAs90NTLeaD",
      "metadata": {
        "id": "lwAs90NTLeaD"
      },
      "source": [
        "- En la siguiente celda, modifique la transformación `train_transform` para incluir técnicas de data augmentation. Debe incluir al menos 2 técnicas de data augmentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "He2ioGe6_RY5",
      "metadata": {
        "id": "He2ioGe6_RY5"
      },
      "outputs": [],
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomCrop(256),\n",
        "    transforms.Resize(256),\n",
        "    transforms.RandomHorizontalFlip(p=0.5), # p=1 fuerza la prob de voltear la imagen\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "\n",
        "eval_transform = transforms.Compose([\n",
        "    transforms.RandomCrop(256),\n",
        "    transforms.Resize(256),\n",
        "    transforms.RandomHorizontalFlip(p=1.0), # p=1 fuerza la prob de voltear la imagen\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yg0MIDEzEYQr",
      "metadata": {
        "id": "yg0MIDEzEYQr"
      },
      "source": [
        "- ¿Por qué eligió esas transformaciones? ¿Son compatibles con la tarea que busca aprender el modelo `pix2pix` o pueden ser contraproducentes en el entrenamiento?\n",
        "> **Respuesta:**\n",
        "1. RandomHorizontalFlip() solo da vuelta la imagen sin modificarla mayormente, es compatible con pix2pix ya que con la modificación hecha a la clase PairedTransformations nos aseguramos de que las transformaciones se apliquen de igual forma a ambas imágenes por lo que no debería generar problemas.\n",
        "2. RandomCrop() permite extraer una región aleatoria de tamaño fijo de la imagen, además dado que la transformación no afecta el alineamiento ya que se puede aplicar de manera sincronizada a los pares de imágenes lo que la hace compatible con pix2pix.\n",
        "\n",
        "- ¿Cuál es el objetivo de la transformación `transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))`? ¿Por qué esta normalización se aplica al final?\n",
        "> **Respuesta:**\n",
        "`transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))` busca aplicar una normalización de media y desviación estándar igual a 0.5 en cada canal. La primera tupla de valores da cuenta de las medias a considerar por cada canal, mientras que la segunda tupla da cuenta de las desviaciones estándar.\n",
        "La normalización actúa sobre *tensores*, no sobre imágenes y, dado que en el penúltimo paso se transformó a tensor, entonces corresponde que el último paso sea normalizar.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5dd88fa6",
      "metadata": {
        "id": "5dd88fa6"
      },
      "outputs": [],
      "source": [
        "train_dataset = MapsDataset(root_dir='/content/data/maps/train', transform=train_transform)\n",
        "eval_dataset = MapsDataset(root_dir='/content/data/maps/val', transform=eval_transform)\n",
        "\n",
        "# Test:\n",
        "x0, x1 = train_dataset[0]\n",
        "assert x0.shape == x1.shape == (3, 256, 256)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=24, shuffle=True, drop_last=True, num_workers=2, pin_memory=True)\n",
        "eval_dataloader = DataLoader(eval_dataset, batch_size=24, shuffle=False, drop_last=True, num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f'Dataset de entrenamiento: {len(train_dataset)} muestras.')\n",
        "print(f'Dataset de validación: {len(eval_dataset)} muestras.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "N7t0NH0AHUuP",
      "metadata": {
        "id": "N7t0NH0AHUuP"
      },
      "source": [
        "- ¿Cuál es la función de los parámetros `num_workers` y `pin_memory` al definir los dataloaders? ¿Por qué se utiliza `drop_last=True`?\n",
        "> **Respuesta:**\n",
        "- *num_workers*: permite especificar el número de procesos en paralelo que se ejecutarán en el proceso de carga de datos. -> Ventaja: agiliza la velocidad de carga.\n",
        "- *pin_workers*: Los tensores son cargado a memoria fija, lo que acelera la copia de datos a CPU.\n",
        "- *Drop_last*: se utiliza como \"buena práctica\" para evitar que se pueda tener un batch incompleto, por ejemplo un batch de 1 imagen en lugar de 10 imágenes.\n",
        "\n",
        "### Visualización\n",
        "\n",
        "En la siguiente celda, la función `show_batch` permite verificar que las implementaciones realizadas hasta el momento son correctas (i.e., las técnicas de data augmentation se aplican de forma sincronizada sobre $(x_0,x_1)$ y son compatibles con la tarea que se busca que aprenda el modelo). Es importante notar que para visualizar imágenes de este dataset, es necesario deshacer la normalización `transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))` para que los valores de los tensores estén en los rangos esperado. Esto se logra con la función `denormalize`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9141008",
      "metadata": {
        "id": "a9141008"
      },
      "outputs": [],
      "source": [
        "def denormalize(batch):\n",
        "    return (batch + 1).clamp(0, 2) / 2\n",
        "\n",
        "def show_batch(x0, x1, x1_pred=None, n_images=4):\n",
        "\n",
        "    x0 = denormalize(x0)\n",
        "    x1 = denormalize(x1)\n",
        "    x1_pred = denormalize(x1_pred) if x1_pred is not None else None\n",
        "\n",
        "    n_cols = 3 if x1_pred is not None else 2\n",
        "    plt.figure(figsize=(n_cols * 2, 2 * n_images))\n",
        "\n",
        "    for n in range(n_images):\n",
        "\n",
        "        # Imagen original:\n",
        "        plt.subplot(n_images, n_cols, n_cols * n + 1)\n",
        "        plt.imshow(x0[n].permute(1, 2, 0))\n",
        "        plt.axis('off')\n",
        "        plt.title(r'$x_0$')\n",
        "\n",
        "        # Transformación esperada:\n",
        "        plt.subplot(n_images, n_cols, n_cols * n + 2)\n",
        "        plt.imshow(x1[n].permute(1, 2, 0))\n",
        "        plt.axis('off')\n",
        "        plt.title(r'$x_1$')\n",
        "\n",
        "        # Transformación predicha:\n",
        "        if x1_pred is not None:\n",
        "            plt.subplot(n_images, n_cols, n_cols * n + 3)\n",
        "            plt.imshow(x1_pred[n].permute(1, 2, 0))\n",
        "            plt.axis('off')\n",
        "            plt.title(r'$\\tilde x_1$')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a37fa066",
      "metadata": {
        "id": "a37fa066"
      },
      "outputs": [],
      "source": [
        "x0, x1 = next(iter(train_dataloader))\n",
        "show_batch(x0, x1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7PW0DXYzXFVb",
      "metadata": {
        "id": "7PW0DXYzXFVb"
      },
      "source": [
        "## Parte 2 (redes neuronales)\n",
        "\n",
        "En esta segunda parte se implementarán las redes neuronales asociadas al generador $G$ (U-Net adaptada para GANs) y al discriminador $D$ (PatchGAN) del modelo `pix2pix`.\n",
        "\n",
        "Es importante destacar que, si bien la formulación del modelo `pix2pix` indica que el generador $G$ depende tanto de la imagen inicial $x_0$ como la variable latente $z$, en la práctica es usual que la red neuronal asociada a $G$ solo reciba $x_0$, mientras que la aleatoriedad suele ser inyectada usando dropout.\n",
        "\n",
        "Por otro lado, dado que las GANs suelen tener un entrenamiento inestables debido a su dinámica de entrenamiento adversativa, es usual tener que incluir heurísticas tanto en la arquitectura como en la optimización. Para efectos de esta tarea, la efectividad de las heurísticas utilizadas se puede decidir por inspección visual de las curvas de entrenamiento y de las imágenes generadas.\n",
        "\n",
        "### Generador\n",
        "\n",
        "- Modifique las clases `ConvBlock`, `DownBlock`, `UpBlock` y/o `UNet` de la [arquitectura U-Net estudiada en clases](https://github.com/fernando-fetis/MDS7203/blob/main/Clases/Clase%2011/notebooks/unet.ipynb) para que incluya dropout y algunas heurísticas de arquitectura que ayuden a estabilizar el entrenamiento. Para esto, puede probar con heurísticas que crea que deberían funcionar bien (i.e., ensayo/error) o con heurísticas que ya se conozcan de otros trabajos (p.g., [DCGAN](https://arxiv.org/abs/1511.06434))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6b77c88",
      "metadata": {
        "id": "c6b77c88"
      },
      "outputs": [],
      "source": [
        "class ConvBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        ...\n",
        "\n",
        "    def forward(self, x):\n",
        "        ...\n",
        "\n",
        "class DownBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        ...\n",
        "\n",
        "    def forward(self, x):\n",
        "        ...\n",
        "\n",
        "class UpBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, dropout=0.5):\n",
        "        super().__init__()\n",
        "        ...\n",
        "\n",
        "    def forward(self, x, skip):\n",
        "        ...\n",
        "\n",
        "class UNet(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, n_classes, base_ch=64):\n",
        "        super().__init__()\n",
        "\n",
        "        ...\n",
        "\n",
        "    def forward(self, x):\n",
        "        ..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8da4b4f",
      "metadata": {
        "id": "f8da4b4f"
      },
      "source": [
        "- ¿Cuáles fueron las heurísticas que incluyó? ¿Por qué utilizó estas heurísticas?\n",
        "> **Respuesta:**\n",
        "\n",
        "### Discriminador\n",
        "\n",
        "Para modelar el discriminador $D$, se utilizara la siguiente red convolucional llamada _PatchGAN_:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bd05f75",
      "metadata": {
        "id": "3bd05f75"
      },
      "outputs": [],
      "source": [
        "class PatchGAN(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(in_channels * 2, 64, kernel_size=4, stride=2, padding=1, bias=True),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=True),\n",
        "            nn.InstanceNorm2d(128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=True),\n",
        "            nn.InstanceNorm2d(256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(256, 512, kernel_size=4, stride=1, padding=1, bias=True),\n",
        "            nn.InstanceNorm2d(512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1, bias=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x0, x1):\n",
        "        x = torch.cat([x0, x1], dim=1)\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qAlD9YCXPhBL",
      "metadata": {
        "id": "qAlD9YCXPhBL"
      },
      "source": [
        "- ¿Cuál es la diferencia principal entre la arquitectura PatchGAN y un clasificador de imágenes usual? ¿Por qué es mejor usar una PatchGAN para `pix2pix`?\n",
        "> **Respuesta:**\n",
        "La diferencia es que patchgan no intenta clasificar toda la imagen como real o falsa de una sola vez, sino que evalúa por zonas si los pedazos de la imagen generada parecen reales o falsas. saca un mapa de probabilidades que le dice al generador en qué partes se ve más fake.\n",
        "Una cnn clásica como las de clasificación normal reduce todo con max pooling y flatten hasta llegar a una predicción global (como perro o gato por ejemplo), pero pierde detalles chicos.\n",
        "Para pix2pix se prefiere usar patchgan porque ayuda a la detección de errores locales, como una textura mal hecha o una sombra rara. Eso es importante para poder corregir esos detalles finos en el training.\n",
        "\n",
        "## Parte 3 (entrenamiento y generación)\n",
        "\n",
        "En esta última parte se entrenará el modelo `pix2pix` implementado en la parte 2 utilizando el dataset implementado en la parte 1.\n",
        "\n",
        "### Inicialización\n",
        "\n",
        "La siguiente función se utilizará para inicializar los parámetros de las redes definidas anteriormente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "krojmOqIU-q1",
      "metadata": {
        "id": "krojmOqIU-q1"
      },
      "outputs": [],
      "source": [
        "def init_weights(module):\n",
        "\n",
        "    if isinstance(module, (nn.Conv2d, nn.ConvTranspose2d)):\n",
        "        nn.init.normal_(module.weight.data, 0.0, 0.02)\n",
        "\n",
        "    elif isinstance(module, nn.InstanceNorm2d):\n",
        "        if module.weight is not None:\n",
        "            nn.init.normal_(module.weight.data, 1.0, 0.02)\n",
        "        if module.bias is not None:\n",
        "            nn.init.constant_(module.bias.data, 0.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KVkUsHkzVpMX",
      "metadata": {
        "id": "KVkUsHkzVpMX"
      },
      "source": [
        "### Función de costo\n",
        "\n",
        "Si bien el método de entrenamiento para el modelo `pix2pix` es similar al de una GAN estándar, es necesario adaptar el cálculo de la función de pérdida a la función de pérdida utilizada por `pix2pix`. Recordar que la función de pérdida usada por este modelo viene dada por\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(G,D)=\n",
        "\\mathbb{E}_{x_0,x_1}\\left[\\log D(x_0,x_1)\\right] + \\mathbb{E}_{x_0,z}\\left[\\log \\left(1-D(x_0,G(x_0,z))\\right)\\right]\n",
        "+ \\lambda\\cdot\\mathbb{E}_{x_0,x_1,z}\\left[\\left\\|x_1-G(x_0,z)\\right\\|_1\\right],\n",
        "$$\n",
        "\n",
        "donde $x_0$ es la imagen que se busca transformar, $x_1$ es la transformación real y $z$ es la variable latente del modelo generativo $G$ (i.e., $\\tilde x_1=G(x_0,z)$ es la transformación de $x_0$ aprendida por el modelo generador $G$). El hiperparámetro $\\lambda>0$ es un escalar positivo.\n",
        "\n",
        "- ¿En qué se diferencia esta función objetivo de la función objetivo de una GAN clásica?\n",
        "> **Respuesta:**\n",
        "a diferencia de una GAN clásica, esta función tiene un hiperparámetro\n",
        "λ>0 que le dice al generador qué tanto debe preocuparse porque la imagen generada se parezca a la original, no solo que se vea real. Además como se usa una pérdida L1 (pixel a pixel), si el lambda es alto entonces el modelo se preocupará más de detalles pequeños, es decir locales como pequeños bordes o texturas, pero si es pequeño ese hiperparámetro entonces se buscará de que la imagen en general parezca real, aunque tenga errores chicos como sombras mal puestas o zonas corridas, por ejemplo que un perro tenga una textura rara en la nariz\n",
        "\n",
        "- ¿Qué evalúa cada término de $\\mathcal{L}(G,D)$? ¿Cuál es el motivo para preferir la función de pérdida $L^1$ en vez de la función de pérdida $L^2$ en el término $\\mathbb{E}_{x_0,x_1,z}\\left[\\left\\|x_1-G(x_0,z)\\right\\|_1\\right]$?\n",
        "> **Respuesta:**\n",
        "Se prefiere usar  la pérdida L1 por sobre la L2 porque permite definir mejor los bordes en la imagen que se genera, ya que, castiga los errores sin ser tan estricto. Además es más robusta ante los outliers, mientras que L2 penaliza mucho los errores grandes. Entonces el modelo, en vez de marcar bien los contrastes (como de negro [0] a blanco [255]), termine suavizando y usando valores intermedios, lo que suaviza bordes y se ve mas borroso o difuminado.\n",
        "- ¿Qué trade-off se genera al variar el hiperparámetro $\\lambda$?\n",
        "> **Respuesta:**\n",
        "Si es lambda tiene un valor muy alto entonces al entrenar la pérdida buscará que pixel a pixel sea igual a la imagen original con la que se entreno, el problema es que al enfocarse tanto en detalles hace que pierda el realismo a nivel general, porque se fijará menos en pasar como imagen real, ya que, buscará tomar valores promedios en pixeles, para no arriesgarse, perdiendo texturas globales. Pero si es bajo lambda entonces los detalles finos no serán tomados en cuenta, porque el generador buscará que la imagen pase como real a pesar de que algunos detalles como pequeñas sombras o zonas estén corridas\n",
        "- ¿En qué se diferencia esta función de pérdida de la función de pérdida usada en [CycleGAN](https://arxiv.org/abs/1703.10593)?\n",
        "> **Respuesta:**\n",
        "Se entrenan diferente, porque pix2pix es aprendizaje supervisado, porque necesita imagenes para poder aprender patrones por ejemplo un zapato en blanco y negro y uno pintado, estando etiquetado cada uno. Pero con CycleGan no se necesita tener imagenes de referencias, sino que se necesita saber pasar por ejemplo de un estilo de dibujo como anime a uno realista y viceversa a través de muchas imagenes. Para lo anterior se necesitan 2 discriminadores y dos generadores, para pasar de un estilo  A (anime) --> B(realista) y de el estilo B(realista)--> A (anime) , en teoría debería ser similar la imagen que sale generada de estilo anime a la que pasa por el ciclo completo A --> B --> 'A'.\n",
        "Por eso se usa además una perdida de ciclo extra para que lo generado  'A' se parezca lo más posible a lo real A\n",
        "\n",
        "\n",
        "\n",
        "Como se comentó anteriormente, en la práctica, es usual considerar $G(x_0,z)=G(x_0)$ (i.e., no se incluye explícitamente una variable latente en la implementación). Además, al calcular la función de pérdida $\\mathcal{L}(G,D)$ para optimizar $G$, se suele usar $- \\mathbb{E}_{x_0}\\left[\\log D(x_0,G(x_0))\\right]$ en vez de $\\mathbb{E}_{x_0}\\left[\\log \\left(1-D(x_0,G(x_0))\\right)\\right]$.\n",
        "\n",
        "- ¿Por qué se prefiere optimizar esta cantidad? ¿Por qué solo se utiliza esto para el generador y no para el discriminador?\n",
        "> **Respuesta:**\n",
        "\n",
        "### Bucle de entrenamiento\n",
        "\n",
        "- Complete la siguiente función de entrenamiento (puede basarse en la [función de entrenamiento vista en clases](https://github.com/fernando-fetis/MDS7203/blob/main/Clases/Clase%209/notebooks/DCGAN.ipynb)) para entrenar el modelo `pix2pix` utilizando $\\mathcal{L}(G,D)$ con las modificaciones mencionadas. Averigüe sobre las decisiones de diseño (optimizadores, schedulers, valores para $\\lambda$) que funcionan bien para entrenar modelos tipo GANs. Si prefiere, puede incluir heurísticas que ayuden en el entrenamiento de las GANs (p.g. inyectar ruido a las imágenes o a las etiquetas). No olvide guardar los `state_dict` de $G$ y $D$ en la parte de early stopping."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b454e26b",
      "metadata": {
        "id": "b454e26b"
      },
      "outputs": [],
      "source": [
        "def train_pix2pix(G, D, train_dataloader, val_dataloader, n_epochs, init_fn=init_weights, patience=10):\n",
        "\n",
        "    # Redes neuronales:\n",
        "    G = G.to(DEVICE).apply(init_fn)\n",
        "    D = D.to(DEVICE).apply(init_fn)\n",
        "\n",
        "    # Optimizadores:\n",
        "    optimizer_G = ...\n",
        "    optimizer_D = ...\n",
        "\n",
        "    # Schedulers:\n",
        "    scheduler_G  = ...\n",
        "    scheduler_D  = ...\n",
        "\n",
        "    dataloaders  = {'train': train_dataloader, 'eval': val_dataloader}\n",
        "\n",
        "    losses = {'train': {'G': [], 'D': []}, 'eval': {'G': [], 'D': []}}\n",
        "    training_log = {'G': None, 'D': None, 'losses': losses}\n",
        "\n",
        "    # Entrenamiento y validación:\n",
        "    try:\n",
        "        for epoch in range(n_epochs):\n",
        "\n",
        "            for mode in ('train', 'eval'):\n",
        "\n",
        "                G.train(mode == 'train')\n",
        "                D.train(mode == 'train')\n",
        "\n",
        "                sum_loss_G, sum_loss_D = 0.0, 0.0\n",
        "\n",
        "                for x0, x1 in tqdm(dataloaders[mode], desc=f'Época {epoch+1}/{n_epochs} [{mode}]', leave=False):\n",
        "                    x0, x1 = x0.to(DEVICE), x1.to(DEVICE)\n",
        "\n",
        "                    with torch.set_grad_enabled(mode == 'train'):\n",
        "                        loss_D = ...\n",
        "                        loss_G = ...\n",
        "\n",
        "                    sum_loss_G += loss_G.item()\n",
        "                    sum_loss_D += loss_D.item()\n",
        "\n",
        "                losses[mode]['G'].append(sum_loss_G / len(dataloaders[mode]))\n",
        "                losses[mode]['D'].append(sum_loss_D / len(dataloaders[mode]))\n",
        "\n",
        "                if mode == 'train':\n",
        "                    scheduler_G.step()\n",
        "                    scheduler_D.step()\n",
        "\n",
        "            # Early stopping:\n",
        "            ...\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print('Entrenamiento interrumpido.')\n",
        "\n",
        "    torch.save(training_log, 'training.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ivUPrBlKdGMh",
      "metadata": {
        "id": "ivUPrBlKdGMh"
      },
      "source": [
        "### Entrenamiento\n",
        "\n",
        "Se entrenará el modelo usando la función anterior. Notar que puede ser necesario redefinir el `batch_size` de los dataloaders para no tener problemas de memoria. Se recomienda probar la generación con pocas iteraciones (p.g. una época) para verificar si todo está bien implementado antes de realizar el entrenamiento completo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "239f48c6",
      "metadata": {
        "id": "239f48c6"
      },
      "outputs": [],
      "source": [
        "# Redes neuronales:\n",
        "generator = UNet(in_channels=3, n_classes=3)\n",
        "discriminator = PatchGAN(in_channels=3)\n",
        "\n",
        "# Entrenamiento:\n",
        "train_pix2pix(generator, discriminator, train_dataloader, eval_dataloader, n_epochs=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b75a685",
      "metadata": {
        "id": "7b75a685"
      },
      "source": [
        "Se visualizarán las curvas de entrenamiento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2eff6215",
      "metadata": {
        "id": "2eff6215"
      },
      "outputs": [],
      "source": [
        "ckpt = torch.load('training.pt', map_location=DEVICE)\n",
        "generator.load_state_dict(ckpt['G'])\n",
        "discriminator.load_state_dict(ckpt['D'])\n",
        "\n",
        "fig, axs = plt.subplots(1, 2, figsize=(16, 5))\n",
        "for ax, model in zip(axs, ('G', 'D')):\n",
        "    ax.plot(ckpt['losses']['train'][model], label='Entrenamiento')\n",
        "    ax.plot(ckpt['losses']['eval'][model], label='Validación')\n",
        "    ax.set_title(f'Entrenamiento para {model}')\n",
        "    ax.set_xlabel('Época')\n",
        "    ax.set_ylabel('Loss')\n",
        "    ax.legend()\n",
        "    ax.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MipeePCrj8e_",
      "metadata": {
        "id": "MipeePCrj8e_"
      },
      "source": [
        "- Intente explicar el comportamiento que observa en las funciones de pérdida del generador y del discriminador.\n",
        "> **Respuesta:**\n",
        "\n",
        "### Generación\n",
        "\n",
        "Con el modelo entrenado, se comparará la transformación generada $\\tilde x_1$ con la transformación real $x_1$ para algunas muestras $x_0$ del conjunto de validación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e97a26b1",
      "metadata": {
        "id": "e97a26b1"
      },
      "outputs": [],
      "source": [
        "def generate(generator, x0):\n",
        "\n",
        "    generator.eval()\n",
        "    x0 = x0.to(DEVICE)\n",
        "    G = generator.to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        x1_pred = G(x0)\n",
        "\n",
        "    return x1_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0BvOAdOOfmwy",
      "metadata": {
        "id": "0BvOAdOOfmwy"
      },
      "outputs": [],
      "source": [
        "x0, x1 = next(iter(eval_dataloader))\n",
        "\n",
        "x1_pred = generate(generator, x0).detach().cpu()\n",
        "show_batch(x0, x1, x1_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zTu2K7V_lWSt",
      "metadata": {
        "id": "zTu2K7V_lWSt"
      },
      "source": [
        "Si el modelo fue entrenado correctamente, las imágenes `x1` y `x1_pred` deben ser similares."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}